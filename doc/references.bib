
@inproceedings{newcombe_kinectfusion:_2011,
	title = {{KinectFusion}: Real-Time Dense Surface Mapping and Tracking},
	url = {http://research.microsoft.com/apps/pubs/default.aspx?id=155378},
	abstract = {{\textless}p{\textgreater}We present a system for accurate real-time mapping of complex and arbitrary indoor scenes in variable lighting conditions, using only a moving low-cost depth camera and commodity graphics hardware. We fuse all of the depth data streamed from a Kinect sensor into a single global implicit surface model of the observed scene in real-time. The current sensor pose is simultaneously obtained by tracking the live depth frame relative to the global model using a coarse-to-fine iterative closest point ({ICP}) algorithm, which uses all of the observed depth data available. We demonstrate the advantages of tracking against the growing full surface model compared with frame-to-frame tracking, obtaining tracking and mapping results in constant time within room sized scenes with limited drift and high accuracy. We also show both qualitative and quantitative results relating to various aspects of our tracking and mapping system. Modelling of natural scenes, in real-time with only commodity sensor and {GPU} hardware, promises an exciting step forward in augmented reality ({AR}), in particular, it allows dense surfaces to be reconstructed in real-time, with a level of detail and robustness beyond any solution yet presented using passive computer vision.{\textless}/p{\textgreater}},
	booktitle = {{IEEE} {ISMAR}},
	publisher = {{IEEE}},
	author = {Newcombe, Richard A. and Izadi, Shahram and Hilliges, Otmar and Molyneaux, David and Kim, David and Davison, Andrew J. and Kohli, Pushmeet and Shotton, Jamie and Hodges, Steve and Fitzgibbon, Andrew},
	date = {2011-10}
}

@inproceedings{izadi_kinectfusion:_2011,
	title = {{KinectFusion}: Real-time 3D Reconstruction and Interaction Using a Moving Depth Camera},
	url = {http://research.microsoft.com/apps/pubs/default.aspx?id=155416},
	abstract = {{\textless}p{\textgreater}{KinectFusion} enables a user holding and moving a standard Kinect camera to rapidly create detailed 3D reconstructions of an indoor scene. Only the depth data from Kinect is used to track the 3D pose of the sensor and reconstruct, geometrically precise, 3D models of the physical scene in real-time. The capabilities of {KinectFusion}, as well as the novel {GPU}-based pipeline are described in full. We show uses of the core system for low-cost handheld scanning, and geometry-aware augmented reality and physics-based interactions. Novel extensions to the core {GPU} pipeline demonstrate object segmentation and user interaction directly in front of the sensor, without degrading camera tracking or reconstruction. These extensions are used to enable real-time multi-touch interactions anywhere, allowing any planar or non-planar reconstructed physical surface to be appropriated for touch.{\textless}/p{\textgreater}},
	publisher = {{ACM} Symposium on User Interface Software and Technology},
	author = {Izadi, Shahram and Kim, David and Hilliges, Otmar and Molyneaux, David and Newcombe, Richard and Kohli, Pushmeet and Shotton, Jamie and Hodges, Steve and Freeman, Dustin and Davison, Andrew and Fitzgibbon, Andrew},
	date = {2011-10}
}

@online{_documentation_pcl,
	title = {Documentation - Point Cloud Library ({PCL})},
	url = {http://pointclouds.org/documentation/tutorials/using_kinfu_large_scale.php},
	titleaddon = {Using Kinfu Large Scale to generate a textured mesh},
	urldate = {2015-03-03},
	file = {Documentation - Point Cloud Library (PCL):/home/tim/.zotero/zotero/k9ju6bar.default/zotero/storage/E9SA7S7C/using_kinfu_large_scale.html:text/html}
}

@online{_buy_jetson,
	title = {Buy Jetson {TK}1 {DevKit}},
	url = {https://developer.nvidia.com/jetson-tk1},
	urldate = {2015-03-03},
	file = {Buy Jetson TK1 DevKit:/home/tim/.zotero/zotero/k9ju6bar.default/zotero/storage/X7CNI43W/jetson-tk1.html:text/html}
}
